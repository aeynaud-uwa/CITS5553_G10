{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Reload the dataset\n",
    "data = pd.read_csv(\"clean.csv\")\n",
    "\n",
    "# Display basic statistical descriptions\n",
    "data.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "original 1D-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv(\"clean.csv\")\n",
    "\n",
    "# Prepare the data\n",
    "X = data.drop(columns=['act', 'id']).values\n",
    "y = data['act'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert labels to one-hot encoded format\n",
    "lb = LabelBinarizer()\n",
    "y_train = lb.fit_transform(y_train)\n",
    "y_test = lb.transform(y_test)\n",
    "\n",
    "# Define the neural network model\n",
    "model = Sequential()\n",
    "model.add(Dense(100, activation='relu', input_shape=(6,)))  # 6 features as input\n",
    "model.add(Dense(6, activation='softmax'))  # 6 classes\n",
    "\n",
    "# Compile and train the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
    "\n",
    "# Predictions\n",
    "y_pred_probs = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_probs, multi_class='ovr')\n",
    "f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "# Confusion matrix visualization\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(conf_matrix, annot=True, cmap=\"YlGnBu\", fmt=\"d\")\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"ROC-AUC: {roc_auc}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv(\"clean.csv\")\n",
    "\n",
    "# Prepare the data\n",
    "X = data.drop(columns=['act', 'id']).values\n",
    "y = data['act'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply SMOTE on training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Convert labels to one-hot encoded format\n",
    "lb = LabelBinarizer()\n",
    "y_train_smote = lb.fit_transform(y_train_smote)\n",
    "y_test = lb.transform(y_test)\n",
    "\n",
    "# Define the neural network model\n",
    "model = Sequential()\n",
    "model.add(Dense(100, activation='relu', input_shape=(6,)))\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "\n",
    "# Compile and train the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train_smote, y_train_smote, epochs=10, batch_size=32)\n",
    "\n",
    "# Predictions\n",
    "y_pred_probs = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_probs, multi_class='ovr')\n",
    "f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "# Confusion matrix visualization\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(conf_matrix, annot=True, cmap=\"YlGnBu\", fmt=\"d\")\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix after SMOTE')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"ROC-AUC: {roc_auc}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic 1D-CNN+Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer, StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv(\"clean.csv\")\n",
    "\n",
    "# Prepare the data\n",
    "X = data.drop(columns=['act', 'id']).values\n",
    "y = data['act'].values\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert labels to one-hot encoded format\n",
    "lb = LabelBinarizer()\n",
    "y_train = lb.fit_transform(y_train)\n",
    "y_test = lb.transform(y_test)\n",
    "\n",
    "# Define the neural network model\n",
    "model = Sequential()\n",
    "model.add(Dense(100, activation='relu', input_shape=(6,)))  # 6 features as input\n",
    "model.add(Dense(6, activation='softmax'))  # 6 classes\n",
    "\n",
    "# Compile and train the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
    "\n",
    "# Predictions\n",
    "y_pred_probs = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_probs, multi_class='ovr')\n",
    "f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "# Confusion matrix visualization\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(conf_matrix, annot=True, cmap=\"YlGnBu\", fmt=\"d\")\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"ROC-AUC: {roc_auc}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy with Time Window Averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv(\"clean.csv\")\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "data[['rotationRate.x', 'rotationRate.y', 'rotationRate.z', 'userAcceleration.x', 'userAcceleration.y', 'userAcceleration.z']] = scaler.fit_transform(data[['rotationRate.x', 'rotationRate.y', 'rotationRate.z', 'userAcceleration.x', 'userAcceleration.y', 'userAcceleration.z']])\n",
    "\n",
    "# Time Window Averaging\n",
    "window_size = 25\n",
    "\n",
    "def time_window_averaging(data, window_size):\n",
    "    averaged_data = []\n",
    "    for i in range(0, len(data) - window_size + 1, window_size):\n",
    "        window = data[i:i+window_size]\n",
    "        averaged_data.append(window.mean(axis=0))\n",
    "    return pd.DataFrame(averaged_data)\n",
    "\n",
    "data_avg = time_window_averaging(data, window_size)\n",
    "\n",
    "# Prepare data for model\n",
    "def prepare_data_fixed_length(data):\n",
    "    X = data.drop(columns=['act', 'id']).values\n",
    "    y = data['act'].values\n",
    "    \n",
    "    X_sequences = []\n",
    "    y_sequences = []\n",
    "    \n",
    "    for i in range(0, len(X) - sequence_length + 1, sequence_length):\n",
    "        X_sequences.append(X[i:i+sequence_length])\n",
    "        y_sequences.append(y[i])\n",
    "        \n",
    "    return np.array(X_sequences), np.array(y_sequences)\n",
    "\n",
    "sequence_length = 128\n",
    "X_avg, y_avg = prepare_data_fixed_length(data_avg)\n",
    "X_train_avg, X_test_avg, y_train_avg, y_test_avg = train_test_split(X_avg, y_avg, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert labels to one-hot encoded format\n",
    "lb = LabelBinarizer()\n",
    "y_train_avg = lb.fit_transform(y_train_avg)\n",
    "y_test_avg = lb.transform(y_test_avg)\n",
    "\n",
    "# Define the 1D CNN model\n",
    "model_avg = Sequential()\n",
    "model_avg.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(128, 6)))\n",
    "model_avg.add(MaxPooling1D(pool_size=2))\n",
    "model_avg.add(Flatten())\n",
    "model_avg.add(Dense(100, activation='relu'))\n",
    "model_avg.add(Dense(6, activation='softmax'))\n",
    "\n",
    "# Compile and train the model\n",
    "model_avg.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model_avg.fit(X_train_avg, y_train_avg, epochs=10, batch_size=32)\n",
    "\n",
    "# Predictions\n",
    "y_pred_probs_avg = model_avg.predict(X_test_avg)\n",
    "y_pred_avg = np.argmax(y_pred_probs_avg, axis=1)\n",
    "y_true_avg = np.argmax(y_test_avg, axis=1)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy_avg = accuracy_score(y_true_avg, y_pred_avg)\n",
    "roc_auc_avg = roc_auc_score(y_test_avg, y_pred_probs_avg, multi_class='ovr')\n",
    "f1_avg = f1_score(y_true_avg, y_pred_avg, average='weighted')\n",
    "\n",
    "# Confusion matrix visualization for averaged data\n",
    "conf_matrix_avg = confusion_matrix(y_true_avg, y_pred_avg)\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(conf_matrix_avg, annot=True, cmap=\"YlGnBu\", fmt=\"d\")\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix after Time Window Averaging')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Accuracy with Time Window Averaging: {accuracy_avg}\")\n",
    "print(f\"ROC-AUC with Time Window Averaging: {roc_auc_avg}\")\n",
    "print(f\"F1 Score with Time Window Averaging: {f1_avg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CNN model is trained using the original dataset, and then the key metrics are calculated and the confusion matrix is displayed.\n",
    "The data is segmented into fixed-length sequences through a time window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv(\"clean.csv\")\n",
    "\n",
    "# Define a function to prepare data based on fixed length\n",
    "sequence_length = 128\n",
    "def prepare_data_fixed_length(data):\n",
    "    X = data.drop(columns=['act', 'id']).values\n",
    "    y = data['act'].values\n",
    "    \n",
    "    X_sequences = []\n",
    "    y_sequences = []\n",
    "    \n",
    "    for i in range(0, len(X) - sequence_length + 1, sequence_length):\n",
    "        X_sequences.append(X[i:i+sequence_length])\n",
    "        y_sequences.append(y[i])\n",
    "        \n",
    "    return np.array(X_sequences), np.array(y_sequences)\n",
    "\n",
    "X, y = prepare_data_fixed_length(data)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert labels to one-hot encoded format\n",
    "lb = LabelBinarizer()\n",
    "y_train = lb.fit_transform(y_train)\n",
    "y_test = lb.transform(y_test)\n",
    "\n",
    "# Define the 1D CNN model\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(128, 6)))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "\n",
    "# Compile and train the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
    "\n",
    "# Predictions\n",
    "y_pred_probs = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_probs, multi_class='ovr')\n",
    "f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "# Confusion matrix visualization\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(conf_matrix, annot=True, cmap=\"YlGnBu\", fmt=\"d\")\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"ROC-AUC: {roc_auc}\")\n",
    "print(f\"F1 Score: {f1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv(\"clean.csv\")\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "data[['rotationRate.x', 'rotationRate.y', 'rotationRate.z', 'userAcceleration.x', 'userAcceleration.y', 'userAcceleration.z']] = scaler.fit_transform(data[['rotationRate.x', 'rotationRate.y', 'rotationRate.z', 'userAcceleration.x', 'userAcceleration.y', 'userAcceleration.z']])\n",
    "\n",
    "# Prepare data for model\n",
    "def prepare_data_fixed_length(data):\n",
    "    X = data.drop(columns=['act', 'id']).values\n",
    "    y = data['act'].values\n",
    "    \n",
    "    X_sequences = []\n",
    "    y_sequences = []\n",
    "    \n",
    "    for i in range(0, len(X) - sequence_length + 1, sequence_length):\n",
    "        X_sequences.append(X[i:i+sequence_length])\n",
    "        y_sequences.append(y[i])\n",
    "        \n",
    "    return np.array(X_sequences), np.array(y_sequences)\n",
    "\n",
    "sequence_length = 128\n",
    "X, y = prepare_data_fixed_length(data)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert labels to one-hot encoded format\n",
    "lb = LabelBinarizer()\n",
    "y_train = lb.fit_transform(y_train)\n",
    "y_test = lb.transform(y_test)\n",
    "\n",
    "# Define the 1D CNN model\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(128, 6)))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "\n",
    "# Compile and train the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
    "\n",
    "# Predictions\n",
    "y_pred_probs = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_probs, multi_class='ovr')\n",
    "f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "# Confusion matrix visualization\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(conf_matrix, annot=True, cmap=\"YlGnBu\", fmt=\"d\")\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix after Standardization')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"ROC-AUC: {roc_auc}\")\n",
    "print(f\"F1 Score: {f1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time window averaging and model training/evaluation sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv(\"clean.csv\")\n",
    "\n",
    "# Time Window Averaging\n",
    "window_size = 25\n",
    "\n",
    "def time_window_averaging(data, window_size):\n",
    "    averaged_data = []\n",
    "    for i in range(0, len(data) - window_size + 1, window_size):\n",
    "        window = data[i:i+window_size]\n",
    "        averaged_data.append(window.mean(axis=0))\n",
    "    return pd.DataFrame(averaged_data)\n",
    "\n",
    "averaged_data = time_window_averaging(data, window_size)\n",
    "\n",
    "# Prepare data for CNN\n",
    "def prepare_data_fixed_length(data):\n",
    "    X = data.drop(columns=['act', 'id']).values\n",
    "    y = data['act'].values\n",
    "    X_sequences = []\n",
    "    y_sequences = []\n",
    "    for i in range(0, len(X) - 128 + 1, 128):\n",
    "        X_sequences.append(X[i:i+128])\n",
    "        y_sequences.append(y[i])\n",
    "    return np.array(X_sequences), np.array(y_sequences)\n",
    "\n",
    "X_avg, y_avg = prepare_data_fixed_length(averaged_data)\n",
    "X_train_avg, X_test_avg, y_train_avg, y_test_avg = train_test_split(X_avg, y_avg, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert labels to one-hot encoded format\n",
    "lb = LabelBinarizer()\n",
    "y_train_avg = lb.fit_transform(y_train_avg)\n",
    "y_test_avg = lb.transform(y_test_avg)\n",
    "\n",
    "# Define the 1D CNN model\n",
    "model_avg = Sequential()\n",
    "model_avg.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(128, 6)))\n",
    "model_avg.add(MaxPooling1D(pool_size=2))\n",
    "model_avg.add(Flatten())\n",
    "model_avg.add(Dense(100, activation='relu'))\n",
    "model_avg.add(Dense(6, activation='softmax'))\n",
    "\n",
    "# Compile and train the model\n",
    "model_avg.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model_avg.fit(X_train_avg, y_train_avg, epochs=10, batch_size=32)\n",
    "\n",
    "# Predictions\n",
    "y_pred_probs_avg = model_avg.predict(X_test_avg)\n",
    "y_pred_avg = np.argmax(y_pred_probs_avg, axis=1)\n",
    "y_true_avg = np.argmax(y_test_avg, axis=1)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy_avg = accuracy_score(y_true_avg, y_pred_avg)\n",
    "roc_auc_avg = roc_auc_score(y_test_avg, y_pred_probs_avg, multi_class='ovr')\n",
    "f1_avg = f1_score(y_true_avg, y_pred_avg, average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy_avg}\")\n",
    "print(f\"ROC-AUC: {roc_auc_avg}\")\n",
    "print(f\"F1 Score: {f1_avg}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think our data is pure data, and I think we should add some noise. See if the accuracy drops? Can these methods be improved?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv(\"clean.csv\")\n",
    "\n",
    "# Add Gaussian noise to the dataset\n",
    "noise_factor = 0.05\n",
    "data_with_noise = data.copy()\n",
    "data_with_noise[['rotationRate.x', 'rotationRate.y', 'rotationRate.z', 'userAcceleration.x', 'userAcceleration.y', 'userAcceleration.z']] += noise_factor * np.random.randn(data.shape[0], 6)\n",
    "\n",
    "# Save the noisy data to a new CSV\n",
    "data_with_noise.to_csv(\"clean_with_noise.csv\", index=False)\n",
    "\n",
    "# Function to prepare data based on fixed length\n",
    "sequence_length = 128\n",
    "\n",
    "def prepare_data_fixed_length(data):\n",
    "    X = data.drop(columns=['act', 'id']).values\n",
    "    y = data['act'].values\n",
    "    \n",
    "    X_sequences = []\n",
    "    y_sequences = []\n",
    "    \n",
    "    for i in range(0, len(X) - sequence_length + 1, sequence_length):\n",
    "        X_sequences.append(X[i:i+sequence_length])\n",
    "        y_sequences.append(y[i])\n",
    "        \n",
    "    return np.array(X_sequences), np.array(y_sequences)\n",
    "\n",
    "X, y = prepare_data_fixed_length(data_with_noise)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert labels to one-hot encoded format\n",
    "lb = LabelBinarizer()\n",
    "y_train = lb.fit_transform(y_train)\n",
    "y_test = lb.transform(y_test)\n",
    "\n",
    "# Define the 1D CNN model\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(128, 6)))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "\n",
    "# Compile and train the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
    "\n",
    "# Predictions\n",
    "y_pred_probs = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_probs, multi_class='ovr')\n",
    "f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "print(f\"Accuracy (with noise): {accuracy}\")\n",
    "print(f\"ROC-AUC (with noise): {roc_auc}\")\n",
    "print(f\"F1 Score (with noise): {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It first applies normalization on the noisy data, and then performs model training and evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelBinarizer, StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv(\"clean_with_noise.csv\")\n",
    "\n",
    "# Apply standardization\n",
    "scaler = StandardScaler()\n",
    "data[['rotationRate.x', 'rotationRate.y', 'rotationRate.z', 'userAcceleration.x', 'userAcceleration.y', 'userAcceleration.z']] = scaler.fit_transform(data[['rotationRate.x', 'rotationRate.y', 'rotationRate.z', 'userAcceleration.x', 'userAcceleration.y', 'userAcceleration.z']])\n",
    "\n",
    "# Function to prepare data based on fixed length\n",
    "sequence_length = 128\n",
    "def prepare_data_fixed_length(data):\n",
    "    X = data.drop(columns=['act', 'id']).values\n",
    "    y = data['act'].values\n",
    "    \n",
    "    X_sequences = []\n",
    "    y_sequences = []\n",
    "    \n",
    "    for i in range(0, len(X) - sequence_length + 1, sequence_length):\n",
    "        X_sequences.append(X[i:i+sequence_length])\n",
    "        y_sequences.append(y[i])\n",
    "        \n",
    "    return np.array(X_sequences), np.array(y_sequences)\n",
    "\n",
    "X, y = prepare_data_fixed_length(data)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert labels to one-hot encoded format\n",
    "lb = LabelBinarizer()\n",
    "y_train = lb.fit_transform(y_train)\n",
    "y_test = lb.transform(y_test)\n",
    "\n",
    "# Define the 1D CNN model\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(128, 6)))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "\n",
    "# Compile and train the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
    "\n",
    "# Predictions\n",
    "y_pred_probs = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_probs, multi_class='ovr')\n",
    "f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "print(f\"Accuracy (with noise and after standardization): {accuracy}\")\n",
    "print(f\"ROC-AUC (with noise and after standardization): {roc_auc}\")\n",
    "print(f\"F1 Score (with noise and after standardization): {f1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noise does have a negative impact on the performance of the model.\n",
    "Normalization is a beneficial preprocessing step that partially counteracts the effect of noise.\n",
    "But even with normalization, the performance does not return to the level of the original data. This indicates that the noise does disturb the meaningful patterns in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The time window is averaged over the noisy data, and then the same CNN architecture is used for training and evaluation\n",
    "Temporal window averaging is applied to smooth the data.\n",
    "The CNN model is used for training.\n",
    "Evaluate the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv(\"clean_with_noise.csv\")\n",
    "\n",
    "# Time Window Averaging\n",
    "window_size = 25\n",
    "\n",
    "def time_window_averaging(data, window_size):\n",
    "    averaged_data = []\n",
    "    for i in range(0, len(data) - window_size + 1, window_size):\n",
    "        window = data[i:i+window_size]\n",
    "        averaged_data.append(window.mean(axis=0))\n",
    "    return pd.DataFrame(averaged_data)\n",
    "\n",
    "averaged_data = time_window_averaging(data, window_size)\n",
    "\n",
    "# Prepare data for CNN\n",
    "def prepare_data_fixed_length(data):\n",
    "    X = data.drop(columns=['act', 'id']).values\n",
    "    y = data['act'].values\n",
    "    X_sequences = []\n",
    "    y_sequences = []\n",
    "    for i in range(0, len(X) - 128 + 1, 128):\n",
    "        X_sequences.append(X[i:i+128])\n",
    "        y_sequences.append(y[i])\n",
    "    return np.array(X_sequences), np.array(y_sequences)\n",
    "\n",
    "X_avg, y_avg = prepare_data_fixed_length(averaged_data)\n",
    "X_train_avg, X_test_avg, y_train_avg, y_test_avg = train_test_split(X_avg, y_avg, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert labels to one-hot encoded format\n",
    "lb = LabelBinarizer()\n",
    "y_train_avg = lb.fit_transform(y_train_avg)\n",
    "y_test_avg = lb.transform(y_test_avg)\n",
    "\n",
    "# Define the 1D CNN model\n",
    "model_avg = Sequential()\n",
    "model_avg.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(128, 6)))\n",
    "model_avg.add(MaxPooling1D(pool_size=2))\n",
    "model_avg.add(Flatten())\n",
    "model_avg.add(Dense(100, activation='relu'))\n",
    "model_avg.add(Dense(6, activation='softmax'))\n",
    "\n",
    "# Compile and train the model\n",
    "model_avg.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model_avg.fit(X_train_avg, y_train_avg, epochs=10, batch_size=32)\n",
    "\n",
    "# Predictions\n",
    "y_pred_probs_avg = model_avg.predict(X_test_avg)\n",
    "y_pred_avg = np.argmax(y_pred_probs_avg, axis=1)\n",
    "y_true_avg = np.argmax(y_test_avg, axis=1)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy_avg = accuracy_score(y_true_avg, y_pred_avg)\n",
    "roc_auc_avg = roc_auc_score(y_test_avg, y_pred_probs_avg, multi_class='ovr')\n",
    "f1_avg = f1_score(y_true_avg, y_pred_avg, average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy_avg}\")\n",
    "print(f\"ROC-AUC: {roc_auc_avg}\")\n",
    "print(f\"F1 Score: {f1_avg}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "standardization and time window averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score\n",
    "\n",
    "# 1. Load the data\n",
    "data = pd.read_csv(\"clean_with_noise.csv\")\n",
    "\n",
    "# 2. Data Standardization\n",
    "scaler = StandardScaler()\n",
    "data[['rotationRate.x', 'rotationRate.y', 'rotationRate.z', 'userAcceleration.x', 'userAcceleration.y', 'userAcceleration.z']] = scaler.fit_transform(data[['rotationRate.x', 'rotationRate.y', 'rotationRate.z', 'userAcceleration.x', 'userAcceleration.y', 'userAcceleration.z']])\n",
    "\n",
    "# 3. Time Window Averaging\n",
    "window_size = 25\n",
    "\n",
    "def time_window_averaging(data, window_size):\n",
    "    averaged_data = []\n",
    "    for i in range(0, len(data) - window_size + 1, window_size):\n",
    "        window = data[i:i+window_size]\n",
    "        averaged_data.append(window.mean(axis=0))\n",
    "    return pd.DataFrame(averaged_data)\n",
    "\n",
    "averaged_data = time_window_averaging(data, window_size)\n",
    "\n",
    "# 4. Prepare data for CNN\n",
    "def prepare_data_fixed_length(data):\n",
    "    X = data.drop(columns=['act', 'id']).values\n",
    "    y = data['act'].values\n",
    "    X_sequences = []\n",
    "    y_sequences = []\n",
    "    for i in range(0, len(X) - 128 + 1, 128):\n",
    "        X_sequences.append(X[i:i+128])\n",
    "        y_sequences.append(y[i])\n",
    "    return np.array(X_sequences), np.array(y_sequences)\n",
    "\n",
    "X_avg, y_avg = prepare_data_fixed_length(averaged_data)\n",
    "X_train_avg, X_test_avg, y_train_avg, y_test_avg = train_test_split(X_avg, y_avg, test_size=0.2, random_state=42)\n",
    "\n",
    "lb = LabelBinarizer()\n",
    "y_train_avg = lb.fit_transform(y_train_avg)\n",
    "y_test_avg = lb.transform(y_test_avg)\n",
    "\n",
    "# 5. Define, train, and evaluate the 1D CNN model\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(128, 6)))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train_avg, y_train_avg, epochs=10, batch_size=32)\n",
    "\n",
    "y_pred_probs_avg = model.predict(X_test_avg)\n",
    "y_pred_avg = np.argmax(y_pred_probs_avg, axis=1)\n",
    "y_true_avg = np.argmax(y_test_avg, axis=1)\n",
    "\n",
    "accuracy_avg = accuracy_score(y_true_avg, y_pred_avg)\n",
    "roc_auc_avg = roc_auc_score(y_test_avg, y_pred_probs_avg, multi_class='ovr')\n",
    "f1_avg = f1_score(y_true_avg, y_pred_avg, average='weighted')\n",
    "\n",
    "print(f\"Accuracy (after standardization and time window averaging): {accuracy_avg}\")\n",
    "print(f\"ROC-AUC (after standardization and time window averaging): {roc_auc_avg}\")\n",
    "print(f\"F1 Score (after standardization and time window averaging): {f1_avg}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will downsample the data using the \"sum\" method. This means that the data points within each time window are accumulated to get a sum value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer, StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv(\"clean_with_noise.csv\")\n",
    "\n",
    "# Downsample data using the sum method\n",
    "window_size = 25\n",
    "def downsample_sum(data, window_size):\n",
    "    summed_data = []\n",
    "    for i in range(0, len(data) - window_size + 1, window_size):\n",
    "        window = data[i:i+window_size]\n",
    "        summed_data.append(window.sum(axis=0))\n",
    "    return pd.DataFrame(summed_data)\n",
    "\n",
    "summed_data = downsample_sum(data, window_size)\n",
    "\n",
    "# Prepare data for CNN\n",
    "def prepare_data_fixed_length(data):\n",
    "    X = data.drop(columns=['act', 'id']).values\n",
    "    y = data['act'].values\n",
    "    X_sequences = []\n",
    "    y_sequences = []\n",
    "    for i in range(0, len(X) - 128 + 1, 128):\n",
    "        X_sequences.append(X[i:i+128])\n",
    "        y_sequences.append(y[i])\n",
    "    return np.array(X_sequences), np.array(y_sequences)\n",
    "\n",
    "X_sum, y_sum = prepare_data_fixed_length(summed_data)\n",
    "X_train_sum, X_test_sum, y_train_sum, y_test_sum = train_test_split(X_sum, y_sum, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert labels to one-hot encoded format\n",
    "lb = LabelBinarizer()\n",
    "y_train_sum = lb.fit_transform(y_train_sum)\n",
    "y_test_sum = lb.transform(y_test_sum)\n",
    "\n",
    "# Define the 1D CNN model\n",
    "model_sum = Sequential()\n",
    "model_sum.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(128, 6)))\n",
    "model_sum.add(MaxPooling1D(pool_size=2))\n",
    "model_sum.add(Flatten())\n",
    "model_sum.add(Dense(100, activation='relu'))\n",
    "model_sum.add(Dense(6, activation='softmax'))\n",
    "\n",
    "# Compile and train the model\n",
    "model_sum.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model_sum.fit(X_train_sum, y_train_sum, epochs=10, batch_size=32)\n",
    "\n",
    "# Predictions\n",
    "y_pred_probs_sum = model_sum.predict(X_test_sum)\n",
    "y_pred_sum = np.argmax(y_pred_probs_sum, axis=1)\n",
    "y_true_sum = np.argmax(y_test_sum, axis=1)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy_sum = accuracy_score(y_true_sum, y_pred_sum)\n",
    "roc_auc_sum = roc_auc_score(y_test_sum, y_pred_probs_sum, multi_class='ovr')\n",
    "f1_sum = f1_score(y_true_sum, y_pred_sum, average='weighted')\n",
    "\n",
    "print(f\"Accuracy (with noise and after downsampling using sum): {accuracy_sum}\")\n",
    "print(f\"ROC-AUC (with noise and after downsampling using sum): {roc_auc_sum}\")\n",
    "print(f\"F1 Score (with noise and after downsampling using sum): {f1_sum}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of the model drops significantly after downsampling using the summation method. This is because by simply summing, we may lose certain important information in the data, leading to poor performance of the model.We experimented with downsampling with the minimum and saw its impact on model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsample data using the min method\n",
    "def downsample_min(data, window_size):\n",
    "    min_data = []\n",
    "    for i in range(0, len(data) - window_size + 1, window_size):\n",
    "        window = data[i:i+window_size]\n",
    "        min_data.append(window.min(axis=0))\n",
    "    return pd.DataFrame(min_data)\n",
    "\n",
    "min_data = downsample_min(data, window_size)\n",
    "\n",
    "# Prepare data for CNN\n",
    "X_min, y_min = prepare_data_fixed_length(min_data)\n",
    "X_train_min, X_test_min, y_train_min, y_test_min = train_test_split(X_min, y_min, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert labels to one-hot encoded format\n",
    "y_train_min = lb.fit_transform(y_train_min)\n",
    "y_test_min = lb.transform(y_test_min)\n",
    "\n",
    "# Define the 1D CNN model\n",
    "model_min = Sequential()\n",
    "model_min.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(128, 6)))\n",
    "model_min.add(MaxPooling1D(pool_size=2))\n",
    "model_min.add(Flatten())\n",
    "model_min.add(Dense(100, activation='relu'))\n",
    "model_min.add(Dense(6, activation='softmax'))\n",
    "\n",
    "# Compile and train the model\n",
    "model_min.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model_min.fit(X_train_min, y_train_min, epochs=10, batch_size=32)\n",
    "\n",
    "# Predictions\n",
    "y_pred_probs_min = model_min.predict(X_test_min)\n",
    "y_pred_min = np.argmax(y_pred_probs_min, axis=1)\n",
    "y_true_min = np.argmax(y_test_min, axis=1)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy_min = accuracy_score(y_true_min, y_pred_min)\n",
    "roc_auc_min = roc_auc_score(y_test_min, y_pred_probs_min, multi_class='ovr')\n",
    "f1_min = f1_score(y_true_min, y_pred_min, average='weighted')\n",
    "\n",
    "print(f\"Accuracy (with noise and after downsampling using min): {accuracy_min}\")\n",
    "print(f\"ROC-AUC (with noise and after downsampling using min): {roc_auc_min}\")\n",
    "print(f\"F1 Score (with noise and after downsampling using min): {f1_min}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of downsampling using the minimum method improve over the results using the summation method, but are still below the performance of the original unprocessed data. This again illustrates that downsampling may cause us to lose important information and thus degrade the performance of the model.\n",
    "\n",
    "Next, we can try downsampling using the Max approach and see the impact on the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsample data using the max method\n",
    "def downsample_max(data, window_size):\n",
    "    max_data = []\n",
    "    for i in range(0, len(data) - window_size + 1, window_size):\n",
    "        window = data[i:i+window_size]\n",
    "        max_data.append(window.max(axis=0))\n",
    "    return pd.DataFrame(max_data)\n",
    "\n",
    "max_data = downsample_max(data, window_size)\n",
    "\n",
    "# Prepare data for CNN\n",
    "X_max, y_max = prepare_data_fixed_length(max_data)\n",
    "X_train_max, X_test_max, y_train_max, y_test_max = train_test_split(X_max, y_max, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert labels to one-hot encoded format\n",
    "y_train_max = lb.fit_transform(y_train_max)\n",
    "y_test_max = lb.transform(y_test_max)\n",
    "\n",
    "# Define the 1D CNN model\n",
    "model_max = Sequential()\n",
    "model_max.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(128, 6)))\n",
    "model_max.add(MaxPooling1D(pool_size=2))\n",
    "model_max.add(Flatten())\n",
    "model_max.add(Dense(100, activation='relu'))\n",
    "model_max.add(Dense(6, activation='softmax'))\n",
    "\n",
    "# Compile and train the model\n",
    "model_max.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model_max.fit(X_train_max, y_train_max, epochs=10, batch_size=32)\n",
    "\n",
    "# Predictions\n",
    "y_pred_probs_max = model_max.predict(X_test_max)\n",
    "y_pred_max = np.argmax(y_pred_probs_max, axis=1)\n",
    "y_true_max = np.argmax(y_test_max, axis=1)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy_max = accuracy_score(y_true_max, y_pred_max)\n",
    "roc_auc_max = roc_auc_score(y_test_max, y_pred_probs_max, multi_class='ovr')\n",
    "f1_max = f1_score(y_true_max, y_pred_max, average='weighted')\n",
    "\n",
    "print(f\"Accuracy (with noise and after downsampling using max): {accuracy_max}\")\n",
    "print(f\"ROC-AUC (with noise and after downsampling using max): {roc_auc_max}\")\n",
    "print(f\"F1 Score (with noise and after downsampling using max): {f1_max}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downsampling with the maximum method results in an improvement over the previous method, but is still below the performance of the original unprocessed data. This suggests that the downsampling method may cause us to lose important information and thus degrade the performance of the model.\n",
    "\n",
    "So far, we have tried three different downsampling methods (sum, min, and Max). Among all these methods, the performance using the maximum value method is the best, but it is still inferior to the original data. This may be because downsampling methods inherently lose some of the information in the original data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Smoothing - moving average\n",
    "Moving average is a commonly used method for data smoothing, especially in time series analysis. It works by computing a moving average of the data points, usually using a fixed window size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv(\"clean_with_noise.csv\")\n",
    "\n",
    "# Apply sliding average (rolling mean) to the data\n",
    "rolling_window_size = 5\n",
    "data_rolling = data[['rotationRate.x', 'rotationRate.y', 'rotationRate.z', 'userAcceleration.x', 'userAcceleration.y', 'userAcceleration.z']].rolling(window=rolling_window_size).mean()\n",
    "data_rolling['act'] = data['act']\n",
    "data_rolling['id'] = data['id']\n",
    "data_rolling = data_rolling.dropna()  # Drop NaN values due to rolling\n",
    "\n",
    "# Function to prepare data based on fixed length\n",
    "sequence_length = 128\n",
    "def prepare_data_fixed_length(data):\n",
    "    X = data.drop(columns=['act', 'id']).values\n",
    "    y = data['act'].values\n",
    "    X_sequences = []\n",
    "    y_sequences = []\n",
    "    for i in range(0, len(X) - sequence_length + 1, sequence_length):\n",
    "        X_sequences.append(X[i:i+sequence_length])\n",
    "        y_sequences.append(y[i])\n",
    "    return np.array(X_sequences), np.array(y_sequences)\n",
    "\n",
    "X_rolling, y_rolling = prepare_data_fixed_length(data_rolling)\n",
    "X_train_rolling, X_test_rolling, y_train_rolling, y_test_rolling = train_test_split(X_rolling, y_rolling, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert labels to one-hot encoded format\n",
    "lb = LabelBinarizer()\n",
    "y_train_rolling = lb.fit_transform(y_train_rolling)\n",
    "y_test_rolling = lb.transform(y_test_rolling)\n",
    "\n",
    "# Define the 1D CNN model\n",
    "model_rolling = Sequential()\n",
    "model_rolling.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(128, 6)))\n",
    "model_rolling.add(MaxPooling1D(pool_size=2))\n",
    "model_rolling.add(Flatten())\n",
    "model_rolling.add(Dense(100, activation='relu'))\n",
    "model_rolling.add(Dense(6, activation='softmax'))\n",
    "\n",
    "# Compile and train the model\n",
    "model_rolling.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model_rolling.fit(X_train_rolling, y_train_rolling, epochs=10, batch_size=32)\n",
    "\n",
    "# Predictions\n",
    "y_pred_probs_rolling = model_rolling.predict(X_test_rolling)\n",
    "y_pred_rolling = np.argmax(y_pred_probs_rolling, axis=1)\n",
    "y_true_rolling = np.argmax(y_test_rolling, axis=1)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy_rolling = accuracy_score(y_true_rolling, y_pred_rolling)\n",
    "roc_auc_rolling = roc_auc_score(y_test_rolling, y_pred_probs_rolling, multi_class='ovr')\n",
    "f1_rolling = f1_score(y_true_rolling, y_pred_rolling, average='weighted')\n",
    "\n",
    "print(f\"Accuracy (with sliding average): {accuracy_rolling}\")\n",
    "print(f\"ROC-AUC (with sliding average): {roc_auc_rolling}\")\n",
    "print(f\"F1 Score (with sliding average): {f1_rolling}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After preprocessing using the sliding average, the model achieves an accuracy of 91.58%, which is fairly close to the accuracy of the original data without noise. This indicates that moving average is an effective method to deal with time series data with noise and contributes to the performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code preprocesses the data using a frequency domain transformation, which is then trained using a CNN model, and outputs a visualization of accuracy, ROC-AUC, F1 score, and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.fft import fft\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the data with noise\n",
    "data_with_noise = pd.read_csv(\"clean_with_noise.csv\")\n",
    "\n",
    "# FFT Transformation\n",
    "sequence_length = 128\n",
    "\n",
    "def prepare_data_fft(data):\n",
    "    X = data.drop(columns=['act', 'id']).values\n",
    "    y = data['act'].values\n",
    "    X_fft = []\n",
    "    y_fft = []\n",
    "    \n",
    "    for i in range(0, len(X) - sequence_length + 1, sequence_length):\n",
    "        transformed_data = np.abs(fft(X[i:i+sequence_length], axis=0))\n",
    "        X_fft.append(transformed_data)\n",
    "        y_fft.append(y[i])\n",
    "        \n",
    "    return np.array(X_fft), np.array(y_fft)\n",
    "\n",
    "X_fft, y_fft = prepare_data_fft(data_with_noise)\n",
    "X_train_fft, X_test_fft, y_train_fft, y_test_fft = train_test_split(X_fft, y_fft, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert labels to one-hot encoded format\n",
    "lb = LabelBinarizer()\n",
    "y_train_fft = lb.fit_transform(y_train_fft)\n",
    "y_test_fft = lb.transform(y_test_fft)\n",
    "\n",
    "# Define the 1D CNN model for FFT data\n",
    "model_fft = Sequential()\n",
    "model_fft.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(128, 6)))\n",
    "model_fft.add(MaxPooling1D(pool_size=2))\n",
    "model_fft.add(Flatten())\n",
    "model_fft.add(Dense(100, activation='relu'))\n",
    "model_fft.add(Dense(6, activation='softmax'))\n",
    "\n",
    "# Compile and train the model\n",
    "model_fft.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model_fft.fit(X_train_fft, y_train_fft, epochs=10, batch_size=32)\n",
    "\n",
    "# Predictions\n",
    "y_pred_probs_fft = model_fft.predict(X_test_fft)\n",
    "y_pred_fft = np.argmax(y_pred_probs_fft, axis=1)\n",
    "y_true_fft = np.argmax(y_test_fft, axis=1)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy_fft = accuracy_score(y_true_fft, y_pred_fft)\n",
    "roc_auc_fft = roc_auc_score(y_test_fft, y_pred_probs_fft, multi_class='ovr')\n",
    "f1_fft = f1_score(y_true_fft, y_pred_fft, average='weighted')\n",
    "\n",
    "print(f\"Accuracy (with FFT): {accuracy_fft}\")\n",
    "print(f\"ROC-AUC (with FFT): {roc_auc_fft}\")\n",
    "print(f\"F1 Score (with FFT): {f1_fft}\")\n",
    "\n",
    "# Confusion matrix visualization\n",
    "conf_matrix_fft = confusion_matrix(y_true_fft, y_pred_fft)\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(conf_matrix_fft, annot=True, cmap=\"YlGnBu\", fmt=\"d\")\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix (FFT)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Fast Fourier Transform (FFT) has given us slightly improved results compared to the original noisy dataset and even some other preprocessing methods. This highlights the potential of FFT in extracting useful frequency-domain features from time-series data, which can be especially beneficial for certain types of activities or signals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original Data (without noise):\n",
    "Accuracy: 0.9235\n",
    "ROC-AUC: 0.9910\n",
    "F1 Score: 0.9211\n",
    "Noisy Data:\n",
    "Accuracy: 0.9076\n",
    "ROC-AUC: 0.9857\n",
    "F1 Score: 0.9077\n",
    "After Standardization:\n",
    "Accuracy: 0.8877\n",
    "ROC-AUC: 0.9831\n",
    "F1 Score: 0.8875\n",
    "Time Window Averaging:\n",
    "Accuracy: 0.8989\n",
    "ROC-AUC: 0.9843\n",
    "F1 Score: 0.8976\n",
    "Sliding Average (moving average):\n",
    "Accuracy: 0.9158\n",
    "ROC-AUC: 0.9898\n",
    "F1 Score: 0.9156\n",
    "FFT:\n",
    "Accuracy: 0.9162\n",
    "ROC-AUC: 0.9910\n",
    "F1 Score: 0.9166\n",
    "It's interesting to see that FFT preprocessing provided slightly better results in terms of accuracy and F1 Score. However, the differences are minor, and depending on the application or real-world use case, one might choose a simpler preprocessing method for ease of implementation and interpretability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
